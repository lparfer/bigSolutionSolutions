{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANLYSIS SPANISH AIRLINESS\n",
    "## Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import collections\n",
    "from collections import defaultdict\n",
    "\n",
    "import datetime\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')\n",
    "from nltk import ngrams\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import WordPunctTokenizer, word_tokenize\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import re\n",
    "\n",
    "import scipy.sparse as sp\n",
    "\n",
    "try:\n",
    "    import seaborn as sns\n",
    "except:\n",
    "    print('Seaborn must be installed to continue (pip install seaborn).\\nIt provides an enhanced plotting experience', \n",
    "          file=sys.stderr)\n",
    "    if input('Do you want me to do it for you? (y/n) ') == 'y':\n",
    "        !pip install seaborn\n",
    "    import seaborn as sns\n",
    "\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier, Perceptron, RidgeClassifier, SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier, NearestCentroid\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
    "\n",
    "import string\n",
    "\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    from wordcloud import WordCloud, STOPWORDS\n",
    "except:\n",
    "    print('For the next cells you need WordCloud (pip install wordcloud)', file=sys.stderr)\n",
    "    if input('Do you want me to do it for you? (y/n) ') == 'y':\n",
    "        !pip install wordcloud\n",
    "        \n",
    "    from wordcloud import WordCloud, STOPWORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precleaning(tweet):\n",
    "\n",
    "    stop_words = set(stopwords.words('spanish'))\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    pat1 = r'@[A-Za-z0-9_]+'\n",
    "    pat2 = r'https?://[^ ]+'\n",
    "    combined_pat = r'|'.join((pat1, pat2))\n",
    "    www_pat = r'www.[^ ]+'\n",
    "\n",
    "    soup = BeautifulSoup(tweet, 'lxml')\n",
    "    souped = soup.get_text()\n",
    "    try:\n",
    "        bom_removed = souped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        bom_removed = souped\n",
    "        \n",
    "    stripped = re.sub(combined_pat, '', bom_removed)\n",
    "    stripped = re.sub(www_pat, '', stripped)\n",
    "    only_letters = re.sub(\"[^a-zA-Z]\", \" \",stripped) \n",
    "    tokens = nltk.word_tokenize(only_letters)[2:]\n",
    "    lower_case = [l.lower() for l in tokens]\n",
    "    \n",
    "    filtered_result = list(filter(lambda l: l not in stop_words, lower_case))\n",
    "    lemmas = [wordnet_lemmatizer.lemmatize(t) for t in filtered_result]\n",
    "    \n",
    "    return lemmas\n",
    "\n",
    "def ngrams(input_list):\n",
    "    \n",
    "    bigrams = [' '.join(t) for t in list(zip(input_list, input_list[1:]))]\n",
    "    trigrams = [' '.join(t) for t in list(zip(input_list, input_list[1:], input_list[2:]))]\n",
    "    \n",
    "    return bigrams+trigrams\n",
    "\n",
    "def count_words(input):\n",
    "    cnt = collections.Counter()\n",
    "    for row in input:\n",
    "        for word in row:\n",
    "            cnt[word] += 1\n",
    "            \n",
    "    return cnt\n",
    "\n",
    "def cleaning(tweet):\n",
    "    \n",
    "    tok = WordPunctTokenizer()\n",
    "    pat1 = r'@[A-Za-z0-9]+'\n",
    "    pat2 = r'https?://[A-Za-z0-9./]+'\n",
    "    pat3=r'(\\w+:\\/\\/\\S+)'\n",
    "    combined_pat = r'|'.join((pat1, pat2,pat3))\n",
    "\n",
    "    soup = BeautifulSoup(tweet, 'lxml')\n",
    "    souped = soup.get_text()\n",
    "    stripped = re.sub(combined_pat, '', souped)\n",
    "    try:\n",
    "        clean = stripped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        clean = stripped\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", clean)\n",
    "    lower_case = letters_only.lower()\n",
    "    # During the letters_only process two lines above, it has created unnecessay white spaces,\n",
    "    # I will tokenize and join together to remove unneccessary white spaces\n",
    "    words = tok.tokenize(lower_case)\n",
    "    return (\" \".join(words)).strip()\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text,language='spanish')\n",
    "    stems = []\n",
    "    for item in tokens:\n",
    "        stems.append(nltk.PorterStemmer().stem(item))\n",
    "    return stems\n",
    "\n",
    "def obtain_data_representation(df, test=None):\n",
    "    \n",
    "    df['text_clean'] = np.array([cleaning(tweet) for tweet in df['text']])\n",
    "\n",
    "    df.text_clean.head(10)\n",
    "    \n",
    "    # If there is no test data, split the input\n",
    "    if test is None:\n",
    "        # Divide data in train and test\n",
    "        train, test = train_test_split(df, test_size=0.25)\n",
    "        df.airline_sentiment = pd.Categorical(df.airline_sentiment)\n",
    "    else:\n",
    "        # Otherwise, all is train\n",
    "        train = df\n",
    "        test['text_clean'] = np.array([cleaning(tweet) for tweet in test['text'] ])\n",
    "        (test.text_clean.head(10))  \n",
    "      \n",
    "    # Create a Bag of Words (BoW), by using train data only\n",
    "    #cv = CountVectorizer(max_features=200,stop_words='english')\n",
    "    cv = TfidfVectorizer(tokenizer=tokenize, max_df = 0.8, use_idf=True,min_df=1,)\n",
    "    x_train = cv.fit_transform(train['text_clean'])\n",
    "    y_train = train['airline_sentiment'].values\n",
    "    \n",
    "    # Obtain BoW for the test data, using the previously fitted one\n",
    "    x_test = cv.transform(test['text_clean'])\n",
    "    try:\n",
    "        y_test = test['airline_sentiment'].values\n",
    "    except:\n",
    "        # It might be the submision file, where we don't have target values\n",
    "        y_test = None\n",
    "        \n",
    "    return {\n",
    "        'train': {\n",
    "            'x': x_train,\n",
    "            'y': y_train\n",
    "        },\n",
    "        'test': {\n",
    "            'x': x_test,\n",
    "            'y': y_test\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "def obtain_raw_data_representation(df, test=None):\n",
    "    \n",
    "    df['text_clean'] =  df['text']\n",
    "\n",
    "    df.text_clean.head(10)\n",
    "    \n",
    "    # If there is no test data, split the input\n",
    "    if test is None:\n",
    "        # Divide data in train and test\n",
    "        train, test = train_test_split(df, test_size=0.25)\n",
    "        df.airline_sentiment = pd.Categorical(df.airline_sentiment)\n",
    "    else:\n",
    "        # Otherwise, all is train\n",
    "        train = df\n",
    "        test['text_clean'] = np.array(test['text'])\n",
    "        (test.text_clean.head(10))  \n",
    "      \n",
    "    # Create a Bag of Words (BoW), by using train data only\n",
    "    #cv = CountVectorizer(max_features=200,stop_words='english')\n",
    "    cv = TfidfVectorizer(tokenizer=tokenize, max_df = 0.8, use_idf=True,min_df=1,)\n",
    "    x_train = cv.fit_transform(train['text_clean'])\n",
    "    y_train = train['airline_sentiment'].values\n",
    "    \n",
    "    # Obtain BoW for the test data, using the previously fitted one\n",
    "    x_test = cv.transform(test['text_clean'])\n",
    "    try:\n",
    "        y_test = test['airline_sentiment'].values\n",
    "    except:\n",
    "        # It might be the submision file, where we don't have target values\n",
    "        y_test = None\n",
    "        \n",
    "    return {\n",
    "        'train': {\n",
    "            'x': x_train,\n",
    "            'y': y_train\n",
    "        },\n",
    "        'test': {\n",
    "            'x': x_test,\n",
    "            'y': y_test\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "### Model training\n",
    "\n",
    "def train_model(dataset, dmodel, *model_args, **model_kwargs):\n",
    "    model = dmodel(*model_args, **model_kwargs)\n",
    "    model.fit(dataset['train']['x'], dataset['train']['y'])\n",
    "    y_pred = model.predict(dataset['test']['x'])\n",
    "    if dataset['test']['y'] is not None:\n",
    "        score = accuracy_score(dataset['test']['y'], y_pred)\n",
    "        print(\"Model score is: {}\".format(score))\n",
    "\n",
    "    return model, y_pred\n",
    "\n",
    "def train_modelMNB(dataset):\n",
    "    # Create a Multinomial Naive Bayes model\n",
    "    model = MultinomialNB(alpha=0.5)  \n",
    "    model.fit(dataset['train']['x'], dataset['train']['y'])\n",
    "    y_pred = model.predict(dataset['test']['x'])\n",
    "    if dataset['test']['y'] is not None:\n",
    "        score = accuracy_score(dataset['test']['y'], y_pred)\n",
    "        print(\"Model MNB score is: {}\".format(score))\n",
    "       \n",
    "    return model, y_pred\n",
    "\n",
    "def train_modelRC(dataset):\n",
    "    # Create a Ridge Classifier model\n",
    "    model = RidgeClassifier(tol=1e-2, solver=\"sag\")\n",
    "    model.fit(dataset['train']['x'], dataset['train']['y'])\n",
    "    y_pred = model.predict(dataset['test']['x'])\n",
    "    if dataset['test']['y'] is not None:\n",
    "        score = accuracy_score(dataset['test']['y'], y_pred)\n",
    "        print(\"Model RC score is: {}\".format(score))\n",
    "\n",
    "    return model, y_pred\n",
    "\n",
    "def train_modelP(dataset):\n",
    "    # Create a Perceptron model\n",
    "    model = Perceptron(n_iter=50)\n",
    "    model.fit(dataset['train']['x'], dataset['train']['y'])\n",
    "    y_pred = model.predict(dataset['test']['x'])\n",
    "    if dataset['test']['y'] is not None:\n",
    "        score = accuracy_score(dataset['test']['y'], y_pred)\n",
    "        print(\"Model P score is: {}\".format(score))\n",
    "      \n",
    "    return model, y_pred\n",
    "\n",
    "def train_modelPAC(dataset):\n",
    "    # Create a Passive Aggressive Classifier model\n",
    "    model = PassiveAggressiveClassifier(n_iter=50)\n",
    "    model.fit(dataset['train']['x'], dataset['train']['y'])\n",
    "    y_pred = model.predict(dataset['test']['x'])\n",
    "    if dataset['test']['y'] is not None:\n",
    "        score = accuracy_score(dataset['test']['y'], y_pred)\n",
    "        print(\"Model PAC score is: {}\".format(score))\n",
    "\n",
    "    return model, y_pred\n",
    "\n",
    "def train_modelRFC(dataset):\n",
    "    # Create a Random Forest model\n",
    "    model = RandomForestClassifier(n_estimators=30)\n",
    "    model.fit(dataset['train']['x'], dataset['train']['y'])\n",
    "    y_pred = model.predict(dataset['test']['x'])\n",
    "    if dataset['test']['y'] is not None:\n",
    "        score = accuracy_score(dataset['test']['y'], y_pred)\n",
    "        print(\"Model RFC score is: {}\".format(score))\n",
    "\n",
    "    return model, y_pred\n",
    "\n",
    "\n",
    "def train_modelRFC2(dataset):\n",
    "    # Create a Random Forest model\n",
    "    model = RandomForestClassifier(n_estimators=30, max_depth=None, min_samples_split=2, random_state=0)\n",
    "    model.fit(dataset['train']['x'], dataset['train']['y'])\n",
    "    y_pred = model.predict(dataset['test']['x'])\n",
    "    if dataset['test']['y'] is not None:\n",
    "        score = accuracy_score(dataset['test']['y'], y_pred)\n",
    "        print(\"Model RFC score is: {}\".format(score))\n",
    "\n",
    "    return model, y_pred\n",
    "\n",
    "def train_modelGBC(dataset):\n",
    "    # Create a Gradient Boosting Classifier\n",
    "    model = GradientBoostingClassifier(n_estimators=100)\n",
    "    model.fit(dataset['train']['x'], dataset['train']['y'])\n",
    "    y_pred = model.predict(dataset['test']['x'])\n",
    "    if dataset['test']['y'] is not None:\n",
    "        score = accuracy_score(dataset['test']['y'], y_pred)\n",
    "        print(\"Model GBC2 score is: {}\".format(score))\n",
    "\n",
    "    return model, y_pred\n",
    "\n",
    "def train_modelGBC2(dataset):\n",
    "    # Create a Gradient Boosting Classifier\n",
    "    model = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "                                       max_depth=1, random_state=0)\n",
    "    model.fit(dataset['train']['x'], dataset['train']['y'])\n",
    "    y_pred = model.predict(dataset['test']['x'])\n",
    "    if dataset['test']['y'] is not None:\n",
    "        score = accuracy_score(dataset['test']['y'], y_pred)\n",
    "        print(\"Model GBC2 score is: {}\".format(score))\n",
    "\n",
    "    return model, y_pred\n",
    "\n",
    "def train_modelETC(dataset):\n",
    "    # Create a Extra Trees Classifier\n",
    "    model = ExtraTreesClassifier(n_estimators=30)\n",
    "    model.fit(dataset['train']['x'], dataset['train']['y'])\n",
    "    y_pred = model.predict(dataset['test']['x'])\n",
    "    if dataset['test']['y'] is not None:\n",
    "        score = accuracy_score(dataset['test']['y'], y_pred)\n",
    "        print(\"Model ETC score is: {}\".format(score))\n",
    "\n",
    "    return model, y_pred\n",
    "\n",
    "def train_modelETC2(dataset):\n",
    "    # Create a Extra Trees Classifier\n",
    "    model = ExtraTreesClassifier(n_estimators=30, max_depth=None, min_samples_split=2, random_state=0)\n",
    "    model.fit(dataset['train']['x'], dataset['train']['y'])\n",
    "    y_pred = model.predict(dataset['test']['x'])\n",
    "    if dataset['test']['y'] is not None:\n",
    "        score = accuracy_score(dataset['test']['y'], y_pred)\n",
    "        print(\"Model ETC2 score is: {}\".format(score))\n",
    "\n",
    "    return model, y_pred\n",
    "\n",
    "def train_modelDTC(dataset):\n",
    "    # Create a Decision Tree Classifier\n",
    "    model = DecisionTreeClassifier()\n",
    "    model.fit(dataset['train']['x'], dataset['train']['y'])\n",
    "    y_pred = model.predict(dataset['test']['x'])\n",
    "    if dataset['test']['y'] is not None:\n",
    "        score = accuracy_score(dataset['test']['y'], y_pred)\n",
    "        print(\"Model DTC score is: {}\".format(score))\n",
    "\n",
    "    return model, y_pred\n",
    "\n",
    "def train_modelDTC2(dataset):\n",
    "    # Create a Decision Tree Classifier\n",
    "    model = DecisionTreeClassifier(max_depth=None, min_samples_split=2, random_state=0)\n",
    "    model.fit(dataset['train']['x'], dataset['train']['y'])\n",
    "    y_pred = model.predict(dataset['test']['x'])\n",
    "    if dataset['test']['y'] is not None:\n",
    "        score = accuracy_score(dataset['test']['y'], y_pred)\n",
    "        print(\"Model DTC2 score is: {}\".format(score))\n",
    "\n",
    "    return model, y_pred\n",
    "\n",
    "def train_modelSGD(dataset):\n",
    "    # Create a Stochastic Gradient Descent model\n",
    "    model = SGDClassifier(loss='hinge', alpha=0.0001, learning_rate='optimal')\n",
    "    model.fit(dataset['train']['x'], dataset['train']['y'])\n",
    "    y_pred = model.predict(dataset['test']['x'])\n",
    "    if dataset['test']['y'] is not None:\n",
    "        score = accuracy_score(dataset['test']['y'], y_pred)\n",
    "        print(\"Model SGD score is: {}\".format(score))\n",
    "       \n",
    "    return model, y_pred\n",
    "\n",
    "def train_modelSVC(dataset):\n",
    "    # Create a Support Vector Classification model\n",
    "    model = SVC(C=1.0, kernel='rbf', class_weight= 'balanced')\n",
    "    model.fit(dataset['train']['x'], dataset['train']['y'])\n",
    "    y_pred = model.predict(dataset['test']['x'])\n",
    "    if dataset['test']['y'] is not None:\n",
    "        score = accuracy_score(dataset['test']['y'], y_pred)\n",
    "        print(\"Model SVC score is: {}\".format(score))\n",
    "\n",
    "    return model, y_pred\n",
    "\n",
    "def train_modelKNC(dataset):\n",
    "    # Create a K-neighbors Classification model\n",
    "    model = KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='auto', leaf_size=30)\n",
    "    model.fit(dataset['train']['x'], dataset['train']['y'])\n",
    "    y_pred = model.predict(dataset['test']['x'])\n",
    "    if dataset['test']['y'] is not None:\n",
    "        score = accuracy_score(dataset['test']['y'], y_pred)\n",
    "        print(\"Model KNC score is: {}\".format(score))\n",
    "       \n",
    "    return model, y_pred\n",
    "\n",
    "def train_modelNCC(dataset):\n",
    "    # Create a nearest centroid Classification model\n",
    "    model = NearestCentroid(metric='euclidean', shrink_threshold=None)\n",
    "    model.fit(dataset['train']['x'], dataset['train']['y'])\n",
    "    y_pred = model.predict(dataset['test']['x'])\n",
    "    if dataset['test']['y'] is not None:\n",
    "        score = accuracy_score(dataset['test']['y'], y_pred)\n",
    "        print(\"Model NCC score is: {}\".format(score))\n",
    "       \n",
    "    return model, y_pred\n",
    "\n",
    "def train_modelGPC(dataset):\n",
    "    # Create a Gaussian Process Classification model\n",
    "    model = GaussianProcessClassifier(kernel=None, optimizer='fmin_l_bfgs_b')\n",
    "    model.fit(dataset['train']['x'], dataset['train']['y'])\n",
    "    y_pred = model.predict(dataset['test']['x'])\n",
    "    if dataset['test']['y'] is not None:\n",
    "        score = accuracy_score(dataset['test']['y'], y_pred)\n",
    "        print(\"Model GPC score is: {}\".format(score))\n",
    "       \n",
    "    return model, y_pred\n",
    "\n",
    "def train_modelGaussianNB(dataset):\n",
    "    # Create a Gaussian Naive Bayes Classification model\n",
    "    model = GaussianNB(priors=None)\n",
    "    model.fit(dataset['train']['x'], dataset['train']['y'])\n",
    "    y_pred = model.predict(dataset['test']['x'])\n",
    "    if dataset['test']['y'] is not None:\n",
    "        score = accuracy_score(dataset['test']['y'], y_pred)\n",
    "        print(\"Model GNB score is: {}\".format(score))\n",
    "        \n",
    "    return model, y_pred\n",
    "\n",
    "def train_modelDTC(dataset):\n",
    "    # Create a Decision Trees Classification model\n",
    "    model = DecisionTreeClassifier(criterion='gini', splitter='best', class_weight=\"balanced\", presort=False)\n",
    "    model.fit(dataset['train']['x'], dataset['train']['y'])\n",
    "    y_pred = model.predict(dataset['test']['x'])\n",
    "    if dataset['test']['y'] is not None:\n",
    "        score = accuracy_score(dataset['test']['y'], y_pred)\n",
    "        print(\"Model DTC score is: {}\".format(score))\n",
    "        \n",
    "    return model, y_pred\n",
    "\n",
    "def train_modelABC(dataset):\n",
    "    # Create a Ada Boost Classification model\n",
    "    model = AdaBoostClassifier(base_estimator=None, algorithm='SAMME.R')\n",
    "    model.fit(dataset['train']['x'], dataset['train']['y'])\n",
    "    y_pred = model.predict(dataset['test']['x'])\n",
    "    if dataset['test']['y'] is not None:\n",
    "        score = accuracy_score(dataset['test']['y'], y_pred)\n",
    "        print(\"Model ABC score is: {}\".format(score))\n",
    "        \n",
    "    return model, y_pred\n",
    "\n",
    "def train_modelMLPC(dataset):\n",
    "    # Create a Multi-layer perceptron Classification model\n",
    "    model = MLPClassifier(hidden_layer_sizes=(100, ), activation='relu', solver='adam', alpha=0.0001, \n",
    "                          learning_rate = 'constant', learning_rate_init=0.001)\n",
    "    model.fit(dataset['train']['x'], dataset['train']['y'])\n",
    "    y_pred = model.predict(dataset['test']['x'])\n",
    "    if dataset['test']['y'] is not None:\n",
    "        score = accuracy_score(dataset['test']['y'], y_pred)\n",
    "        print(\"Model MLPC score is: {}\".format(score))\n",
    "        \n",
    "    return model, y_pred\n",
    "\n",
    "### Submision\n",
    "\n",
    "def create_submit_file(df_submission, ypred):\n",
    "    date = datetime.datetime.now().strftime(\"%m_%d_%Y-%H_%M_%S\")\n",
    "    filename = 'submission_' + date + '.csv'\n",
    "    \n",
    "    df_submission['airline_sentiment'] = ypred\n",
    "    df_submission[['airline_sentiment']].to_csv(filename)\n",
    "    \n",
    "    print('Submission file created: {}'.format(filename))\n",
    "    print('Upload it to Kaggle InClass')\n",
    "    \n",
    "def build_bag_of_words_features_filtered(words):\n",
    "    return {\n",
    "        word:1 for word in words \\\n",
    "        if not word in useless_words}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "### Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (20.0, 20.0)\n",
    "matplotlib.rcParams['figure.dpi'] = 200\n",
    "    \n",
    "df = pd.read_csv('tweets_spanish.csv', encoding='utf-16', index_col='tweet_id', sep=',')\n",
    "df.tweet_created = pd.to_datetime(df.tweet_created)\n",
    "head=df[['text','airline_sentiment']]\n",
    "head.head()\n",
    "\n",
    "print(\"Number of tweets:\", df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NaN analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.isnull().any(axis=1)].head()\n",
    "np.sum(df.isnull().any(axis=1))\n",
    "df.isnull().any(axis=0)\n",
    "df.isnull().sum()\n",
    "usertime=2748/len(df)\n",
    "# We eliminate tweet coord, tweet location and user_timezone because there are more than \n",
    "# a 25 % of missing values in those variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweet text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bcdataset_tmp = df # Dataset before cleaning\n",
    "df['text']=df['text'].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "df['text_clean'] = np.array([precleaning(tweet) for tweet in df['text'] ])\n",
    "df['grams'] = df.text_clean.apply(ngrams)\n",
    "df[['grams']].head()\n",
    "clean_df=df[['text_clean','airline_sentiment','grams']]\n",
    "clean_df.to_csv('clean_tweet.csv',encoding='utf-8')\n",
    "csv = 'clean_tweet.csv'\n",
    "my_df = pd.read_csv(csv,index_col=0)\n",
    "my_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description spanish dataset clean\n",
    "#### Most frequent negative tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df.airline_sentiment == 'negative')][['grams']].apply(count_words)['grams'].most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most frequent positive tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df.airline_sentiment == 'positive')][['grams']].apply(count_words)['grams'].most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wordclouds for positive tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tweets = my_df[my_df.airline_sentiment == 'positive']\n",
    "pos_string = []\n",
    "for t in pos_tweets.grams:\n",
    "    pos_string.append(t)\n",
    "    \n",
    "pos_string = pd.Series(pos_string).str.cat(sep=' ')\n",
    "\n",
    "wc = {'width': 600, 'height': 300, 'random_state': 0}\n",
    "wordcloud = WordCloud(**wc).generate(pos_string)\n",
    "\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "ignore = set((\"numero\",\"euro\",\"euro'\",\"suerte\",\"suerte'\",'hola',\"hola'\",'ano',\"ano'\",'deseadme',\"deseadme'\",\n",
    "              \"mejor'\",'destino','destinos','europeos',\"europeos'\",'click',\"click'\",'aeropuerto',\"gracias\",\n",
    "              \"gracias'\",\"aeropuerto'\",'solo',\"solo'\",'hacer',\"hacer'\",'decadas',\"decadas'\",'dia',\"dia'\",\n",
    "              'avion',\"avion'\",'espana',\"espana'\",'madrid',\"madrid'\",\"spanair\",'letal',\"letal'\",'vuelo',\n",
    "              'volar',\"volar'\",\"vuelo'\",'vuelos',\"vuelos'\", \"ma'\",\"ma\",\"q'\",\"si\",\"si'\",\"d'\"))\n",
    "fwc = {'stopwords':ignore,**wc}\n",
    "wordcloud = WordCloud(**fwc).generate(pos_string)\n",
    "\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Wordclouds for negative tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_tweets = my_df[my_df.airline_sentiment == 'negative']\n",
    "neg_string = []\n",
    "for t in neg_tweets.grams:\n",
    "    neg_string.append(t)\n",
    "    \n",
    "neg_string = pd.Series(neg_string).str.cat(sep=' ')\n",
    "\n",
    "wc = {'width': 600, 'height': 300, 'random_state': 0}\n",
    "wordcloud = WordCloud(**wc).generate(neg_string)\n",
    "\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "ignore = set(('aeropuerto',\"aeropuerto'\",'solo',\"solo'\",'hacer',\"hacer'\",'decadas',\"decadas'\",'dia',\"dia'\",\n",
    "              'avion',\"avion'\",'espana',\"espana'\",'madrid',\"madrid'\",\"spanair\",'letal',\"letal'\",'ryanair', \n",
    "              \"ryanair'\",\"iberia'\",'iberia','vuelo','volar',\"volar'\",\"vuelo'\",'vuelos',\"vuelos'\", \"ma'\",\"ma\",\n",
    "              \"q'\",\"si\",\"si'\",\"d'\"))\n",
    "fwc = {'stopwords':ignore,**wc}\n",
    "wordcloud = WordCloud(**fwc).generate(neg_string)\n",
    "\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Porcentaje de positivos y negativos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral_tweets=my_df[my_df.airline_sentiment == 'neutral']\n",
    "perc_pos=len(pos_tweets)/(len(pos_tweets)+len(neg_tweets)+len(neutral_tweets))\n",
    "perc_neg=len(neg_tweets)/(len(pos_tweets)+len(neg_tweets)+len(neutral_tweets))\n",
    "perc_neutral=len(neutral_tweets)/(len(pos_tweets)+len(neg_tweets)+len(neutral_tweets))\n",
    "\n",
    "print(perc_pos,perc_neutral,perc_neg)\n",
    "\n",
    "labels = 'Positive', 'Neutral', 'Negative'\n",
    "pie = [perc_pos,perc_neutral,perc_neg]\n",
    "plt.rcParams['font.size'] = 44.0\n",
    "colors = [sns.xkcd_rgb[\"pale red\"],\n",
    "          sns.xkcd_rgb[\"denim blue\"],\n",
    "          sns.xkcd_rgb[\"medium green\"]]\n",
    "explode = (0.1, 0, 0)  \n",
    "# Plot\n",
    "plt.pie(pie, explode=explode, labels=labels, colors=colors,\n",
    "        autopct='%1.1f%%', shadow=False, radius=0.5)\n",
    " \n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frecuencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments = df.groupby('airline_sentiment').size()\n",
    "sentiments\n",
    "\n",
    "height = [3788, 2555, 1524]\n",
    "bars = ('negative', 'neutral', 'positive')\n",
    "y_pos = np.arange(len(bars))\n",
    "\n",
    "# Plot horizaontally with bars\n",
    "plt.bar(y_pos, height, color=[sns.xkcd_rgb[\"pale red\"],\n",
    "                              sns.xkcd_rgb[\"denim blue\"],\n",
    "                              sns.xkcd_rgb[\"medium green\"]])\n",
    "plt.xticks(y_pos, bars)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tweets per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_by_date = df.set_index('tweet_created')\n",
    "df_date_sent = df_by_date[['airline_sentiment']]\n",
    "\n",
    "sent_onehot = pd.get_dummies(df_date_sent['airline_sentiment'])\n",
    "\n",
    "df_date_sent = pd.concat((df_date_sent, sent_onehot)).drop('airline_sentiment', axis=1)\n",
    "\n",
    "sums = df_date_sent.groupby(pd.TimeGrouper('D')).sum()\n",
    "sm = sums.dropna()\n",
    "# Lines plot\n",
    "ax = sm.plot(color=[sns.xkcd_rgb[\"pale red\"], \n",
    "                      sns.xkcd_rgb[\"denim blue\"], \n",
    "                      sns.xkcd_rgb[\"medium green\"]])\n",
    "ax.set_ylabel('Number of tweets')\n",
    "ax.set_xlabel('Date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "### Obtaining data for building a predictive model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['text_clean'] = np.array([cleaning(tweet) for tweet in df['text'] ])\n",
    "\n",
    "dataset=obtain_data_representation(df)\n",
    "\n",
    "#Generate balanced dataset by collecting same number of twits from each group of \n",
    "# twits - positive, negative and neutral\n",
    "am_pos=len(pos_tweets)\n",
    "am_neg=len(neg_tweets)\n",
    "am_neutral=len(neutral_tweets)\n",
    "\n",
    "print(am_pos)\n",
    "print(am_neg)\n",
    "print(am_neutral)\n",
    "\n",
    "neg_tweets_df=df.loc[df.airline_sentiment == 'negative']\n",
    "neu_tweets_df=df.loc[df.airline_sentiment == 'neutral']\n",
    "pos_tweets_df=df.loc[df.airline_sentiment == 'positive']\n",
    "\n",
    "indeces_negative = neg_tweets_df.index.tolist()\n",
    "indeces_neutral = neu_tweets_df.index.tolist()\n",
    "\n",
    "inds_negs = defaultdict(list)\n",
    "checked_neg = []\n",
    "inds_neus = defaultdict(list)\n",
    "checked_neu = []\n",
    "\n",
    "for_selection_neg = indeces_negative\n",
    "for_selection_neu = indeces_neutral\n",
    "\n",
    "random.shuffle(for_selection_neg)\n",
    "random.shuffle(for_selection_neu)\n",
    "\n",
    "inds_neg = random.sample(for_selection_neg, pos_tweets_df.shape[0])\n",
    "inds_neu = random.sample(for_selection_neu, pos_tweets_df.shape[0])\n",
    "\n",
    "neg_df1 = pd.DataFrame()\n",
    "neu_df1 = pd.DataFrame()\n",
    "balanced_df = pd.DataFrame()\n",
    "\n",
    "print(neg_tweets_df.head())\n",
    "for ng in range(len(inds_neg)):\n",
    "    indneg = inds_neg[ng]\n",
    "    neg_df=neg_tweets_df.iloc[neg_tweets_df.index == indneg]\n",
    "    neg_df1 = neg_df1.append(neg_df)\n",
    "\n",
    "for nu in range(len(inds_neu)):\n",
    "    indneu = inds_neu[nu]\n",
    "    neu_df=neg_tweets_df.iloc[neu_tweets_df.index == indneu]\n",
    "    neu_df1 = neu_df1.append(neu_df)    \n",
    "\n",
    "balanced_df = balanced_df.append(pos_tweets_df)\n",
    "balanced_df = balanced_df.append(neg_df1)\n",
    "balanced_df = balanced_df.append(neu_df1)\n",
    "\n",
    "balanced_df['text_clean'] = np.array([cleaning(tweet) for tweet in balanced_df['text'] ])\n",
    "\n",
    "bdataset=obtain_data_representation(balanced_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training\n",
    "\n",
    "#### Testing models with imbalanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "modelBNB, _ = train_model(dataset, BernoulliNB)\n",
    "modelmn, _ = train_model(dataset,  MultinomialNB)\n",
    "modelrc, _ = train_model(dataset,  RidgeClassifier)\n",
    "modelp, _ = train_model(dataset,  Perceptron)\n",
    "modelpac, _ = train_model(dataset,  PassiveAggressiveClassifier)\n",
    "modelrfc, _ = train_model(dataset,  RandomForestClassifier)\n",
    "modelMNB, _ = train_modelMNB(dataset)\n",
    "modelP, _ = train_modelP(dataset)\n",
    "modelRC, _ = train_modelRC(dataset)\n",
    "modelPAC, _ = train_modelPAC(dataset)\n",
    "modelRFC, _ = train_modelRFC(dataset)\n",
    "modelSGD, _ = train_modelSGD(dataset)\n",
    "modelSVC, _ = train_modelSVC(dataset)\n",
    "modelKNC, _ = train_modelKNC(dataset)\n",
    "modelNCC, _ = train_modelNCC(dataset)\n",
    "modelDTC, _ = train_modelDTC(dataset)\n",
    "modelABC, _ = train_modelABC(dataset)\n",
    "modelETC, _ = train_modelETC(dataset)\n",
    "modelETC2, _ = train_modelETC2(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing models with balanced data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelBNB, _ = train_model(bdataset, BernoulliNB)\n",
    "modelmn, _ = train_model(bdataset,  MultinomialNB)\n",
    "modelrc, _ = train_model(bdataset,  RidgeClassifier)\n",
    "modelp, _ = train_model(bdataset,  Perceptron)\n",
    "modelpac, _ = train_model(bdataset,  PassiveAggressiveClassifier)\n",
    "modelrfc, _ = train_model(bdataset,  RandomForestClassifier)\n",
    "modelMNB, _ = train_modelMNB(bdataset)\n",
    "modelP, _ = train_modelP(bdataset)\n",
    "modelRC, _ = train_modelRC(bdataset)\n",
    "modelPAC, _ = train_modelPAC(bdataset)\n",
    "modelRFC, _ = train_modelRFC(bdataset)\n",
    "modelSGD, _ = train_modelSGD(bdataset)\n",
    "modelSVC, _ = train_modelSVC(bdataset)\n",
    "modelKNC, _ = train_modelKNC(bdataset)\n",
    "modelNCC, _ = train_modelNCC(bdataset)\n",
    "modelDTC, _ = train_modelDTC(bdataset)\n",
    "modelABC, _ = train_modelABC(bdataset)\n",
    "modelETC, _ = train_modelETC(bdataset)\n",
    "modelETC2, _ = train_modelETC2(bdataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing models with data before cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bcdataset = obtain_raw_data_representation(bcdataset_tmp)\n",
    "\n",
    "modelBNB, _ = train_model(bcdataset, BernoulliNB)\n",
    "modelmn, _ = train_model(bcdataset,  MultinomialNB)\n",
    "modelrc, _ = train_model(bcdataset,  RidgeClassifier)\n",
    "modelp, _ = train_model(bcdataset,  Perceptron)\n",
    "modelpac, _ = train_model(bcdataset,  PassiveAggressiveClassifier)\n",
    "modelrfc, _ = train_model(bcdataset,  RandomForestClassifier)\n",
    "modelMNB, _ = train_modelMNB(bcdataset)\n",
    "modelP, _ = train_modelP(bcdataset)\n",
    "modelRC, _ = train_modelRC(bcdataset)\n",
    "modelPAC, _ = train_modelPAC(bcdataset)\n",
    "modelRFC, _ = train_modelRFC(bcdataset)\n",
    "modelSGD, _ = train_modelSGD(bcdataset)\n",
    "modelSVC, _ = train_modelSVC(bcdataset)\n",
    "modelKNC, _ = train_modelKNC(bcdataset)\n",
    "modelNCC, _ = train_modelNCC(bcdataset)\n",
    "modelDTC, _ = train_modelDTC(bcdataset)\n",
    "modelABC, _ = train_modelABC(bcdataset)\n",
    "modelETC, _ = train_modelETC(bcdataset)\n",
    "modelETC2, _ = train_modelETC2(bcdataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
